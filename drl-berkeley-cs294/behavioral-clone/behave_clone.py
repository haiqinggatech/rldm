#!/usr/bin/env python
"""
Behavioral cloning based on expert policy data generated by running
run_expert.py. 
# Date: 3/15/2017
# Author: Tasuku Miura
"""
import os
import argparse
from time import gmtime, strftime

import pickle
import tensorflow as tf
import numpy as np
import csv
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, InputLayer
from keras.optimizers import RMSprop
from keras.callbacks import TensorBoard, CSVLogger
from sklearn.model_selection import train_test_split

from bc_util import *
from bc_callbacks import CloneStats


seed = 777
np.random.seed(seed)


def base_model(input_dim, output_dim):
    model = Sequential()
    model.add(InputLayer(batch_input_shape=(None, input_dim,)))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(output_dim))
    return model

def dnn_model(input_dim, output_dim):
    model = Sequential()
    model.add(InputLayer(batch_input_shape=(None, input_dim,)))
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(output_dim))
    return model 

def lstm_model(batch_size, time_steps, input_dim, output_dim):
    #LSTM unnecessary but wanted to see how the model performed.
    from keras.layers import LSTM
    model = Sequential()
    model.add(LSTM(64, 
                   batch_input_shape=(1, time_steps, input_dim),
                   return_sequences=True,
                   stateful=True))
    model.add(LSTM(64, 
                   batch_input_shape=(1, time_steps, input_dim),
                   stateful=True))
    model.add(Dense(output_dim))
    return model

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('roll_out', type=str)
    parser.add_argument('envname', type=str)
    parser.add_argument('num_rollout', type=int)
    parser.add_argument('log', type=bool)
    args = parser.parse_args()

    

    print('Loading rollouts...')
    data = load_rollout(args.roll_out)

    #split and train and validation.
    xTr, xTe, yTr, yTe = train_test_split(data[0], data[1], test_size=0.2)
    xTrmean, xTrstd = get_data_stats(xTr)
    
    #handle cases where std equals zero.
    xTrstd[np.where(xTrstd == 0.)] = np.random.normal(0,0.1,1) * 10e-10

    xTr = (xTr - xTrmean)/xTrstd
    xTe = (xTe - xTrmean)/xTrstd

    #Get dimensions of input to model and num of actions.
    obs_dim = xTr[0].shape[0]
    act_dim = yTr[0].shape[1]
    
    #set batch size
    batch_size = 128


    #set flag to true, if you are using LSTM.
    lstm_flag = False
    timesteps = 0
    if lstm_flag:
        #TODO: is the shape of data correct?
        #https://keras.io/getting-started/faq/#how-can-i-use-stateful-rnns
        timesteps = 1

        #for statefulness in LSTM
        batch_size = 1

        #reshape inputs, for use in LSTM model
        xTr = lstm_reshape(xTr, timesteps, obs_dim)
        xTe = lstm_reshape(xTe, timesteps, obs_dim)


    #define model - select which model to use.
    #model = base_model(obs_dim, act_dim)
    #model = dnn_model(obs_dim, act_dim)
    #model = lstm_model(batch_size, timesteps, obs_dim, act_dim)

    print('Model initiated') 
    model.compile(optimizer='rmsprop', loss='mse')
    
    #define callbacks 
    #- logs dumped to "./logs/csv" and "./logs/tf" respectively
    csv_logger = CSVLogger('./logs/csv/{}-{}.log'.format(
                            args.envname, 
                            strftime("%Y%m%d-%H-%M-%S",gmtime())))

    tfb_logger = TensorBoard('./logs/tf', write_graph=True)


    #need to pass mean and std of train data for normalization when running
    #rollouts of the agent
    bhclone_logger = CloneStats(args.envname, xTrmean, xTrstd, timesteps)

    if args.log == False:
        cb = [bhclone_logger]
    else:
        cb = [csv_logger, tfb_logger, bhclone_logger]

        
    #train
    history = model.fit_generator(
                    generate_batches(xTr, yTr, batch_size),
                    callbacks=cb,
                    epochs=10,
                    steps_per_epoch=len(xTr)/batch_size,
                    validation_data=generate_batches(xTe, yTe, batch_size),
                    validation_steps=len(xTe))


    #store stats, and dump to file
    bhclone_stats = {'average_reward':np.array(bhclone_logger.ave_reward),
                     'losses': np.array(bhclone_logger.losses),
                     'val_losses': np.array(bhclone_logger.val_losses)
                    }

    if args.log:
        with open("./logs/averewards/averwd-{}-{}-{}.log".format(
                  args.envname,
                  strftime("%Y%m%d-%H-%M-%S", gmtime()),
                  args.num_rollout),'wb') as f: 
            pickle.dump(bhclone_stats, f)


if __name__=='__main__':
    main()

    
            
